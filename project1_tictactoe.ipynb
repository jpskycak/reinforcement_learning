{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef unlist(l):\\n    return [item for sublist in l for item in sublist]\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "'''\n",
    "example state: np.array([['x','x',' '],['o','o',' '],[' ',' ',' ']])\n",
    "example action: {'player': 'x', 'cell': (0,2)}\n",
    "'''\n",
    "\n",
    "def state2board(state, board=\" {0} | {1} | {2} \\n-----------\\n {3} | {4} | {5} \\n-----------\\n {6} | {7} | {8} \"):\n",
    "    return board.format(*state.flatten())\n",
    "\n",
    "def state2hash(state):\n",
    "    return ''.join(state.flatten())\n",
    "\n",
    "def state_transition2newstate(state,transition):\n",
    "    newstate = state.copy()\n",
    "    newstate[transition['action']] = transition['player']\n",
    "    return newstate\n",
    "\n",
    "def state2player(state):\n",
    "    if (state=='x').sum() > (state=='o').sum():\n",
    "        return 'o'\n",
    "    else:\n",
    "        return 'x'\n",
    "    \n",
    "def state2iswin(state):\n",
    "    x = (state == 'x')\n",
    "    o = (state == 'o')\n",
    "    \n",
    "    x_triples = x.prod(axis=0).sum() + x.prod(axis=1).sum() + np.diag(x).prod() + np.diag(x.transpose()).prod()\n",
    "    o_triples = o.prod(axis=0).sum() + o.prod(axis=1).sum() + np.diag(o).prod() + np.diag(o.transpose()).prod()\n",
    "    \n",
    "    if x_triples + o_triples > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def state2actions(state):\n",
    "    if state2iswin(state):\n",
    "        return []\n",
    "    else:\n",
    "        return zip(np.where(state == ' ')[0], np.where(state == ' ')[1])\n",
    "\n",
    "def state2transitions(state):\n",
    "    return [{'player': state2player(state), 'action': action} for action in state2actions(state)]\n",
    "    \n",
    "def level2nextlevel(level):\n",
    "    nextlevel = []\n",
    "    for state in level:\n",
    "        for transition in state2transitions(state):\n",
    "            newstate = state_transition2newstate(state,transition)\n",
    "            if state2hash(newstate) not in [state2hash(s) for s in nextlevel]:\n",
    "                nextlevel.append(newstate)\n",
    "    return nextlevel\n",
    "\n",
    "#def state2num():\n",
    "#def id2state():\n",
    "'''\n",
    "def unlist(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING LEVELS...\n",
      "   level 1 constructed | duration: 0.00131500000001 | num_states: 9\n",
      "   level 2 constructed | duration: 0.038 | num_states: 72\n",
      "   level 3 constructed | duration: 0.350984 | num_states: 252\n",
      "   level 4 constructed | duration: 2.25799 | num_states: 756\n",
      "   level 5 constructed | duration: 11.916864 | num_states: 1260\n",
      "   level 6 constructed | duration: 12.893995 | num_states: 1540\n",
      "   level 7 constructed | duration: 11.306077 | num_states: 1155\n",
      "   level 8 constructed | duration: 1.625487 | num_states: 420\n",
      "   level 9 constructed | duration: 0.102787 | num_states: 84\n"
     ]
    }
   ],
   "source": [
    "print \"BUILDING LEVELS...\"\n",
    "levels = {}\n",
    "levels[0] = [np.array([[' ',' ',' '],[' ',' ',' '],[' ',' ',' ']])]\n",
    "for i in range(9):\n",
    "    start_time = time.clock()\n",
    "    levels[i+1] = level2nextlevel(levels[i])\n",
    "    end_time = time.clock()\n",
    "    print '   level {0} constructed | duration: {1} | num_states: {2}'.format(*[i+1,end_time-start_time,len(levels[i+1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([['x', ' ', ' '],\n",
       "        [' ', ' ', ' '],\n",
       "        [' ', ' ', ' ']], \n",
       "       dtype='|S1'), array([[' ', 'x', ' '],\n",
       "        [' ', ' ', ' '],\n",
       "        [' ', ' ', ' ']], \n",
       "       dtype='|S1'), array([[' ', ' ', 'x'],\n",
       "        [' ', ' ', ' '],\n",
       "        [' ', ' ', ' ']], \n",
       "       dtype='|S1'), array([[' ', ' ', ' '],\n",
       "        ['x', ' ', ' '],\n",
       "        [' ', ' ', ' ']], \n",
       "       dtype='|S1'), array([[' ', ' ', ' '],\n",
       "        [' ', 'x', ' '],\n",
       "        [' ', ' ', ' ']], \n",
       "       dtype='|S1'), array([[' ', ' ', ' '],\n",
       "        [' ', ' ', 'x'],\n",
       "        [' ', ' ', ' ']], \n",
       "       dtype='|S1'), array([[' ', ' ', ' '],\n",
       "        [' ', ' ', ' '],\n",
       "        ['x', ' ', ' ']], \n",
       "       dtype='|S1'), array([[' ', ' ', ' '],\n",
       "        [' ', ' ', ' '],\n",
       "        [' ', 'x', ' ']], \n",
       "       dtype='|S1'), array([[' ', ' ', ' '],\n",
       "        [' ', ' ', ' '],\n",
       "        [' ', ' ', 'x']], \n",
       "       dtype='|S1')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next: build around hash functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif __name__ == \"__main__\":\\n    p1 = Agent(1, lossval = -1)\\n    p2 = Agent(2, lossval = -1)\\n    r1 = Agent(1, learning = False)\\n    r2 = Agent(2, learning = False)\\n    r1.epsilon = 1\\n    r2.epsilon = 1\\n    series = [\\'P1-Win\\',\\'P1-Lose\\',\\'P1-Draw\\',\\'P2-Win\\',\\'P2-Lose\\',\\'P2-Draw\\']\\n    #series = [\\'P1-Win\\', \\'P2-Win\\', \\'Draw\\']\\n    colors = [\\'r\\',\\'b\\',\\'g\\',\\'c\\',\\'m\\',\\'b\\']\\n    markers = [\\'+\\', \\'.\\', \\'o\\', \\'*\\', \\'^\\', \\'s\\']\\n    f = open(\\'results.csv\\', \\'wb\\')\\n    writer = csv.writer(f)    \\n    writer.writerow(series)\\n    perf = [[] for _ in range(len(series) + 1)]\\n    for i in range(10000):\\n        if i % 10 == 0:\\n            print \\'Game: {0}\\'.format(i)\\n            probs = measure_performance_vs_random(p1, p2)\\n            writer.writerow(probs)\\n            f.flush()\\n            perf[0].append(i)\\n            for idx,x in enumerate(probs):\\n                perf[idx+1].append(x)\\n        winner = play(p1,p2)\\n        p1.episode_over(winner)\\n        #winner = play(r1,p2)\\n        p2.episode_over(winner)\\n    f.close()\\n    for i in range(1,len(perf)):\\n        plt.plot(perf[0], perf[i], label=series[i-1], color=colors[i-1])\\n    plt.xlabel(\\'Episodes\\')\\n    plt.ylabel(\\'Probability\\')\\n    plt.title(\\'RL Agent Performance vs. Random Agent\\n({0} loss value, self-play)\\'.format(p1.lossval))\\n    #plt.title(\\'P1 Loss={0} vs. P2 Loss={1}\\'.format(p1.lossval, p2.lossval))\\n    plt.legend()\\n    #plt.show()\\n    #plt.savefig(\\'p1loss{0}vsp2loss{1}.png\\'.format(p1.lossval, p2.lossval))\\n    plt.savefig(\\'selfplay_random_{0}loss.png\\'.format(p1.lossval))\\n    while True:\\n        p2.verbose = True\\n        p1 = Human(1)\\n        winner = play(p1,p2)\\n        p1.episode_over(winner)\\n        p2.episode_over(winner)\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from copy import copy, deepcopy\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EMPTY = 0\n",
    "PLAYER_X = 1\n",
    "PLAYER_O = 2\n",
    "DRAW = 3\n",
    "\n",
    "def emptystate():\n",
    "    return [[EMPTY,EMPTY,EMPTY],[EMPTY,EMPTY,EMPTY],[EMPTY,EMPTY,EMPTY]]\n",
    "\n",
    "def gameover(state):\n",
    "    for i in range(3):\n",
    "        if state[i][0] != EMPTY and state[i][0] == state[i][1] and state[i][0] == state[i][2]:\n",
    "            return state[i][0]\n",
    "        if state[0][i] != EMPTY and state[0][i] == state[1][i] and state[0][i] == state[2][i]:\n",
    "            return state[0][i]\n",
    "    if state[0][0] != EMPTY and state[0][0] == state[1][1] and state[0][0] == state[2][2]:\n",
    "        return state[0][0]\n",
    "    if state[0][2] != EMPTY and state[0][2] == state[1][1] and state[0][2] == state[2][0]:\n",
    "        return state[0][2]\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            if state[i][j] == EMPTY:\n",
    "                return EMPTY\n",
    "    return DRAW\n",
    "\n",
    "def last_to_act(state):\n",
    "    countx = 0\n",
    "    counto = 0\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            if state[i][j] == PLAYER_X:\n",
    "                countx += 1\n",
    "            elif state[i][j] == PLAYER_O:\n",
    "                counto += 1\n",
    "    if countx == counto:\n",
    "        return PLAYER_O\n",
    "    if countx == (counto + 1):\n",
    "        return PLAYER_X\n",
    "    return -1\n",
    "\n",
    "\n",
    "def enumstates(state, idx, agent):\n",
    "    if idx > 8:\n",
    "        player = last_to_act(state)\n",
    "        if player == agent.player:\n",
    "            agent.add(state)\n",
    "    else:\n",
    "        winner = gameover(state)\n",
    "        if winner != EMPTY:\n",
    "            return\n",
    "        i = idx / 3\n",
    "        j = idx % 3\n",
    "        for val in range(3):\n",
    "            state[i][j] = val\n",
    "            enumstates(state, idx+1, agent)\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, player, verbose = False, lossval = 0, learning = True):\n",
    "        self.values = {}\n",
    "        self.player = player\n",
    "        self.verbose = verbose\n",
    "        self.lossval = lossval\n",
    "        self.learning = learning\n",
    "        self.epsilon = 0.1\n",
    "        self.alpha = 0.99\n",
    "        self.prevstate = None\n",
    "        self.prevscore = 0\n",
    "        self.count = 0\n",
    "        enumstates(emptystate(), 0, self)\n",
    "\n",
    "    def episode_over(self, winner):\n",
    "        self.backup(self.winnerval(winner))\n",
    "        self.prevstate = None\n",
    "        self.prevscore = 0\n",
    "\n",
    "    def action(self, state):\n",
    "        r = random.random()\n",
    "        if r < self.epsilon:\n",
    "            move = self.random(state)\n",
    "            self.log('>>>>>>> Exploratory action: ' + str(move))\n",
    "        else:\n",
    "            move = self.greedy(state)\n",
    "            self.log('>>>>>>> Best action: ' + str(move))\n",
    "        state[move[0]][move[1]] = self.player\n",
    "        self.prevstate = self.statetuple(state)\n",
    "        self.prevscore = self.lookup(state)\n",
    "        state[move[0]][move[1]] = EMPTY\n",
    "        return move\n",
    "\n",
    "    def random(self, state):\n",
    "        available = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if state[i][j] == EMPTY:\n",
    "                    available.append((i,j))\n",
    "        return random.choice(available)\n",
    "\n",
    "    def greedy(self, state):\n",
    "        maxval = -50000\n",
    "        maxmove = None\n",
    "        if self.verbose:\n",
    "            cells = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if state[i][j] == EMPTY:\n",
    "                    state[i][j] = self.player\n",
    "                    val = self.lookup(state)\n",
    "                    state[i][j] = EMPTY\n",
    "                    if val > maxval:\n",
    "                        maxval = val\n",
    "                        maxmove = (i, j)\n",
    "                    if self.verbose:\n",
    "                        cells.append('{0:.3f}'.format(val).center(6))\n",
    "                elif self.verbose:\n",
    "                    cells.append(NAMES[state[i][j]].center(6))\n",
    "        if self.verbose:\n",
    "            print BOARD_FORMAT.format(*cells)\n",
    "        self.backup(maxval)\n",
    "        return maxmove\n",
    "\n",
    "    def backup(self, nextval):\n",
    "        if self.prevstate != None and self.learning:\n",
    "            self.values[self.prevstate] += self.alpha * (nextval - self.prevscore)\n",
    "\n",
    "    def lookup(self, state):\n",
    "        key = self.statetuple(state)\n",
    "        if not key in self.values:\n",
    "            self.add(key)\n",
    "        return self.values[key]\n",
    "\n",
    "    def add(self, state):\n",
    "        winner = gameover(state)\n",
    "        tup = self.statetuple(state)\n",
    "        self.values[tup] = self.winnerval(winner)\n",
    "\n",
    "    def winnerval(self, winner):\n",
    "        if winner == self.player:\n",
    "            return 1\n",
    "        elif winner == EMPTY:\n",
    "            return 0.5\n",
    "        elif winner == DRAW:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.lossval\n",
    "\n",
    "    def printvalues(self):\n",
    "        vals = deepcopy(self.values)\n",
    "        for key in vals:\n",
    "            state = [list(key[0]),list(key[1]),list(key[2])]\n",
    "            cells = []\n",
    "            for i in range(3):\n",
    "                for j in range(3):\n",
    "                    if state[i][j] == EMPTY:\n",
    "                        state[i][j] = self.player\n",
    "                        cells.append(str(self.lookup(state)).center(3))\n",
    "                        state[i][j] = EMPTY\n",
    "                    else:\n",
    "                        cells.append(NAMES[state[i][j]].center(3))\n",
    "            print BOARD_FORMAT.format(*cells)\n",
    "\n",
    "    def statetuple(self, state):\n",
    "        return (tuple(state[0]),tuple(state[1]),tuple(state[2]))\n",
    "\n",
    "    def log(self, s):\n",
    "        if self.verbose:\n",
    "            print s\n",
    "\n",
    "class Human(object):\n",
    "    def __init__(self, player):\n",
    "        self.player = player\n",
    "\n",
    "    def action(self, state):\n",
    "        printboard(state)\n",
    "        action = raw_input('Your move? ')\n",
    "        return (int(action.split(',')[0]),int(action.split(',')[1]))\n",
    "\n",
    "    def episode_over(self, winner):\n",
    "        if winner == DRAW:\n",
    "            print 'Game over! It was a draw.'\n",
    "        else:\n",
    "            print 'Game over! Winner: Player {0}'.format(winner)\n",
    "\n",
    "def play(agent1, agent2):\n",
    "    state = emptystate()\n",
    "    for i in range(9):\n",
    "        if i % 2 == 0:\n",
    "            move = agent1.action(state)\n",
    "        else:\n",
    "            move = agent2.action(state)\n",
    "        state[move[0]][move[1]] = (i % 2) + 1\n",
    "        winner = gameover(state)\n",
    "        if winner != EMPTY:\n",
    "            return winner\n",
    "    return winner\n",
    "\n",
    "def measure_performance_vs_random(agent1, agent2):\n",
    "    epsilon1 = agent1.epsilon\n",
    "    epsilon2 = agent2.epsilon\n",
    "    agent1.epsilon = 0\n",
    "    agent2.epsilon = 0\n",
    "    agent1.learning = False\n",
    "    agent2.learning = False\n",
    "    r1 = Agent(1)\n",
    "    r2 = Agent(2)\n",
    "    r1.epsilon = 1\n",
    "    r2.epsilon = 1\n",
    "    probs = [0,0,0,0,0,0]\n",
    "    games = 100\n",
    "    for i in range(games):\n",
    "        winner = play(agent1, r2)\n",
    "        if winner == PLAYER_X:\n",
    "            probs[0] += 1.0 / games\n",
    "        elif winner == PLAYER_O:\n",
    "            probs[1] += 1.0 / games\n",
    "        else:\n",
    "            probs[2] += 1.0 / games\n",
    "    for i in range(games):\n",
    "        winner = play(r1, agent2)\n",
    "        if winner == PLAYER_O:\n",
    "            probs[3] += 1.0 / games\n",
    "        elif winner == PLAYER_X:\n",
    "            probs[4] += 1.0 / games\n",
    "        else:\n",
    "            probs[5] += 1.0 / games\n",
    "    agent1.epsilon = epsilon1\n",
    "    agent2.epsilon = epsilon2\n",
    "    agent1.learning = True\n",
    "    agent2.learning = True\n",
    "    return probs\n",
    "\n",
    "def measure_performance_vs_each_other(agent1, agent2):\n",
    "    #epsilon1 = agent1.epsilon\n",
    "    #epsilon2 = agent2.epsilon\n",
    "    #agent1.epsilon = 0\n",
    "    #agent2.epsilon = 0\n",
    "    #agent1.learning = False\n",
    "    #agent2.learning = False\n",
    "    probs = [0,0,0]\n",
    "    games = 100\n",
    "    for i in range(games):\n",
    "        winner = play(agent1, agent2)\n",
    "        if winner == PLAYER_X:\n",
    "            probs[0] += 1.0 / games\n",
    "        elif winner == PLAYER_O:\n",
    "            probs[1] += 1.0 / games\n",
    "        else:\n",
    "            probs[2] += 1.0 / games\n",
    "    #agent1.epsilon = epsilon1\n",
    "    #agent2.epsilon = epsilon2\n",
    "    #agent1.learning = True\n",
    "    #agent2.learning = True\n",
    "    return probs\n",
    "\n",
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "    p1 = Agent(1, lossval = -1)\n",
    "    p2 = Agent(2, lossval = -1)\n",
    "    r1 = Agent(1, learning = False)\n",
    "    r2 = Agent(2, learning = False)\n",
    "    r1.epsilon = 1\n",
    "    r2.epsilon = 1\n",
    "    series = ['P1-Win','P1-Lose','P1-Draw','P2-Win','P2-Lose','P2-Draw']\n",
    "    #series = ['P1-Win', 'P2-Win', 'Draw']\n",
    "    colors = ['r','b','g','c','m','b']\n",
    "    markers = ['+', '.', 'o', '*', '^', 's']\n",
    "    f = open('results.csv', 'wb')\n",
    "    writer = csv.writer(f)    \n",
    "    writer.writerow(series)\n",
    "    perf = [[] for _ in range(len(series) + 1)]\n",
    "    for i in range(10000):\n",
    "        if i % 10 == 0:\n",
    "            print 'Game: {0}'.format(i)\n",
    "            probs = measure_performance_vs_random(p1, p2)\n",
    "            writer.writerow(probs)\n",
    "            f.flush()\n",
    "            perf[0].append(i)\n",
    "            for idx,x in enumerate(probs):\n",
    "                perf[idx+1].append(x)\n",
    "        winner = play(p1,p2)\n",
    "        p1.episode_over(winner)\n",
    "        #winner = play(r1,p2)\n",
    "        p2.episode_over(winner)\n",
    "    f.close()\n",
    "    for i in range(1,len(perf)):\n",
    "        plt.plot(perf[0], perf[i], label=series[i-1], color=colors[i-1])\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('RL Agent Performance vs. Random Agent\\n({0} loss value, self-play)'.format(p1.lossval))\n",
    "    #plt.title('P1 Loss={0} vs. P2 Loss={1}'.format(p1.lossval, p2.lossval))\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "    #plt.savefig('p1loss{0}vsp2loss{1}.png'.format(p1.lossval, p2.lossval))\n",
    "    plt.savefig('selfplay_random_{0}loss.png'.format(p1.lossval))\n",
    "    while True:\n",
    "        p2.verbose = True\n",
    "        p1 = Human(1)\n",
    "        winner = play(p1,p2)\n",
    "        p1.episode_over(winner)\n",
    "        p2.episode_over(winner)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
